\section{S-L1 Design and Implementation} \label{sec:design}
\subsection{Overview}
S-L1 is a level 1 cache implemented entirely in software.
It uses the space available in each SX's shared memory, which has the same access latency as the hardware L1.
We considered using SX's raster cache, but \todo{... say why we did not use it ...}

The design of S-L1 is based on three key design elements.
First, the cache space is partitioned into thread-private cache segments, each containing one or more cache lines. 
The number of cache lines in each cache segment is determined dynamically at runtime based on the amount of available shared memory, the number of threads running in the SX and the size of the cache line. 
The decision to use thread-private cache segments is based on the fact that all threads execute the same code, and the fact that there is little or no inter-thread data sharing in the streaming applications we are targeting so the threads mostly process data independently in disjoint location of memory.
Allowing all threads to share the entire cache space would likely result in unnecessary collisions.

Secondly, we use relatively small cache lines. 
The optimal cache line size to a large extent depends on the applications' memory access patterns.
Larger cache lines perform better for applications exhibiting high spacial locality,
but they perform poorer for applications with low spacial locality due to ($i$) the extra overhead of loading the cache lines requiring multiple memory transactions and ($ii$) the increased cache thrashing because fewer cache lines are available.
 
We decided on using 16-byte cache lines after experimenting with different cache line sizes --- see Section~\ref{sec:experiments}. 
This size works well because it it is the widest load/store size available on modern GPUs,
allowing the load/store of an entire cache line with one memory access.
Moreover, the use of larger lines is constrained by the fact that the size of shared memory is limited --- the most recent GPUs only have 48KB of shared memory per SX on which up to 2,048 threads can be scheduled.

Thirdly, we only cache a limited number of data structures.
The number of cache lines allocated to each thread, denoted by $CLN$, determines how many data structures we cache.
$CLN$ is calculated at runtime as
\begin{center} 
[$(shMemSizePerSM / numThreadsPerSM) / cacheLineSize$]
\end{center}
where $shMemSizePerSM$ is the amount of shared memory available per SX, $numThreadsPerSM$ is the total number of threads allocated on each SX, and $cacheLineSize$ is the size of the cache line; i.e., 16 bytes in our current design. 

The amount of shared memory available for the S-L1 cache depends on how much shared memory has already been allocated for the application.
The application can allocate shared memory statically or dynamically at run time.
Hence,  a mixed compile-time/runtime approach is required to identify how much memory is allocated by the application.
Whatever space is left over can be used for S-L1.
$NumThreadsPerSM$ is calculated at runtime, in part \todo{Why "in part"?} by using the configuration the programmer specifies at kernel invocation. 

Once the number of cache lines per thread -- $CLN$ -- is determined, up to that number
of data structures are marked as to be cached and a separate cache line is assigned to each.\footnote{
	In principle, multiple cache lines could be assigned to a data structure, 
	but we found this does not benefit  the streaming applications we are targeting.}
Data structures that are not marked will not be cached and are accessed directly from memory. 
If the available size of shared memory per thread is less than $cacheLineSize$ (perhaps because the shared memory is extensively used by the application), no cache lines are assigned to threads (i.e. $CLN = 0$), so the software cache is effectively disabled. 

To determine which data structures to cache, we evaluate the benefit of caching the data of each data structure using a short monitoring phase at runtime.
In the monitoring phase, the core computation of the application is executed for a short period of time, during which the software cache for each data structure is simulated (using DRAM) so that the cache hit rates can be determined.
When the monitoring phase terminates, the $CLN$ data structures with the highest cache hit rates will be marked as to be cached. 
The code required for monitoring phase is injected into existing applications using straightforward compiler transformations. 



\subsection{Code Transformations}
The compiler transforms the main loop(s) of the GPU kernel into two loop slices. 
The first loop slice is used for the monitoring phase, where the computation is executed for a short period of time using the cache simulator.
After the first loop slice terminates, the data structures are ranked based on their corresponding cache hit rates, and the top $CLN$ data structures are selected to be cached in S-L1. 
The second loop slice then executes the remainder of the computation using S-L1 for the top $CLN$  data structures.

The following example shows a sample transformation: \todo{the code needs to be indented}

{\footnotesize

\begin{verbatim}

//Some initialization
for(int i = start; i < end; i ++)
{
   char a = charInput[i];
   int b = intInput[i];
 
   int e = doComputation(a, b);
   intOutput[i] = e;
}
//Some final computation
\end{verbatim}
}
is transformed into:

{\footnotesize
\begin{verbatim}
//Some initialization

cacheConfig_t cacheConfig;
int i = start;

//slice 1: monitoring phase
for(; (i < end) && (counter<THRESHOLD); i ++)
{
  char a = charInput[i];
  simulateCache(&charInput[i], 
       0, &cacheConfig);
  int b = intInput[i];
  simulateCache(&intInput[i], 
       1, &cacheConfig);

  int e = doComputation(a, b);

  intOutput[i] = e;
  simulateCache(&intOutput[i], 
      2, &cacheConfig);

}

calculateWhatDataToCache(&cacheConfig, availNumCacheSegments);

//slice 2: rest of the computation
for(; i < end; i ++)
{
  char a = *((char*) accessThroughCache(
         &charInput[i], 0, &cacheConfig));
  int b = *((int*) accessThroughCache(
         &intInput[i], 1, &cacheConfig));

  int e = doComputation(a, b);

  *((int*) accessThroughCache(
         &intOutput[i], 2, &cacheConfig)) = e;
}
flush(&cacheConfig);
//Some final computation
\end{verbatim}
}

% {\bf Monitoring phase:}
\subsubsection{Monitoring phase}
In the monitoring loop, a call to \texttt{simulateCache()} is inserted after each memory access.
This function takes as argument the address of the memory access, a {\it data structure identifier}, and the reference to a {\it cacheConfig} object, which stores all information collected during the monitoring phase.
The {\it data structure identifier} is the identifier of the data structure accessed in the corresponding memory access. 
This identifier is assigned to each data structure statically at compile time; the compiler identifies individual data structures by extracting the pointers from kernel arguments. 

The pseudo code of \texttt{simulateCache()} is listed below. 
This function keeps track of which data is currently being cached in the cache line,
assuming a single cache line is allocated for each thread and data structure,
and it counts the number of cache hits that occurred.
To do this, \texttt{cacheConfig} contains, for each data structure and thread, an address variable identifying the memory address of the data that would currently be in the cache,
and a counter that is incremented whenever a cache hit would occur.
On a cache miss, the address variable is updated with the memory address of the data that would be loaded into the cache line.

\todo{indent code}

{\footnotesize
\begin{verbatim}
simulateCache(addr, accessId, cacheConfig)
{
   addr /= CACHELINESIZE;

   if(addr == cacheConfig.cacheLine[accessId].addr)
      cacheConfig.cacheLine[accessId].hit ++;
   else
      cacheConfig.cacheLine[accessId].addr = addr;
}
\end{verbatim}
}

The monitoring phase is run until sufficiently many memory accesses have been simulated so that the behavior of the cache can be reliably inferred. 
To do this, we simply count the number of times \texttt{simulateCache()} is called by each thread; once it reaches to a predefined threshold for each thread, the monitoring phase is exited.
This pre-defined threshold is set to 300 in our current implementation.\footnote{
	While this method of statically setting the duration of monitoring phase works 
	well for  regular GPU applications such as the ones we are targeting, 
	more sophisticated methods may be required for more complex,  
	irregular GPU applications. 
	Moreover, while we only run the monitoring phase once, it may be beneficial
	to enter into a monitoring phase multiple times during a long running kernel 
	to adapt to potential changes in the caching behavior during run time.}
% Note: your description below cannot be:
% a flag is set by the \texttt{simulateCache()}
% and the corresponding thread exits the monitoring loop. 




Finally, a call to \texttt{flush()} is inserted right after the loop. 
\todo{The flush is not visible in the code. Moreover, since you only simulated the cache, why would there be modified cache lines???? Doesn't make sense!!! Consider cutting the entire paragraph...}
This function simply flushes the modified cache lines to memory and sets the mapping address associated with all cache line to zero as a way to invalidate all the cache lines.



\subsubsection{\bf Determining what to cache} 
In the general case, we mark the $CLN$ data structures with the highest cache hit rates to be cached in S-L1.
However, there are two exceptions. 
First,  we distinguish between read-only and read-write data structures.
Read-write data structures incur more overhead, since dirty bits need to be maintained and dirty lines need to be written back to memory.
Hence, we give higher priority to read-only data structures when selecting which structures to cache.
Currently, we select a read-write data structure over a read-only data structure only
if its cache hit rate is twice that of the read-only data structure, because accesses to read-write cache lines involve the execution of twice as many instructions on average.

Secondly, in our current implementation, we only cache data structures if they have a cache hit rate above XX\%. 
We do this because otherwise the overhead of the software implementation is not amortized by the gains in average memory access times.
\todo{replace XX and explain why the XX}.

\subsubsection{Computation phase}
In the second loop slice, the compiler replaces all memory accesses with calls to \texttt{accessThroughCache()}. 
This function returns an address, which will either be the address of the data in the cache, or the address of the data in memory, depending on whether the accessed data structure is cached or not. 
\todo{The following sentence is BS and should be cut.}
Note that this corresponds to the C-level representation of the transformations; 
the compiler in-lines calls to \texttt{accessThroughCache()}, so instead of first calling \texttt{accessThroughCache()}and then using the addresses it returns, the data is directly accessed.
A simplified version of the \texttt{accessThroughCache()} is as follows:

{\footnotesize
\begin{verbatim}
void* accessThroughCache(void* addr, int accessId, 
         cacheConfig_t* cacheConfig)
{
  if(cacheConfig.isCached[accessId] == NOT_CACHED)
  {
    return addr;
  }
  else
  {
    //If already cached, then simply return the 
    //address within the cache segment
    if(alreadyCached(addr, cacheConfig.cacheLine[accessId]))
    {
      return &(cacheConfig.cacheSegments[accessId].data[addr % 16]);
    }
    //requested data is not in the cache, so, 
    //before caching it we need to evict current data.
    else 
    {
      //If not dirty, simply overwrite. If dirty, 
      //first dump the dirty data to memory

      if(cacheConfig.cacheSegments[accessId].dirty)
      {
	        dumpToMemory(cacheConfig.cacheSegments[accessId]);
      }
      loadNewData(addr, cacheConfig.cacheSegments[accessId]);
      return &(cacheConfig.cacheSegments[accessId].data[addr % 16]);
    }
  }
}
\end{verbatim}
}

If the data of a cacheable structure is not found in the cache line, while processing a call to \texttt{accessThroughCache()}, then the current contents of the cache line is evicted before new data is loaded into the cache line from memory.  
When evicting a line, its dirty bit is checked if the corresponding data structure is read-write. 
If not set, the cache line is simply overwritten by the new data.
If set, the modified data is written back to memory before loading the new data.
We keep a bitmap to identify which portions of the cache line are modified, so that only those need to be written back; 
this also guarantees that if two different threads cache the same data (in two cache S-L1 lines) and modify different potions, they will not overwrite each others data.
Atomic bitwise operations use the bit mask as a write mask to perform the data write-backs. \todo{check to see if this last sentence is correct}

Extra overhead can be avoided if the pointers to data structures, provided as arguments to the GPU kernel, are not aliased. 
Programmers can indicate this is the case by including the \texttt{restrict} keyword with each kernel argument. 
If this keyword is not included then the caching layer will still works properly, albeit 
with extra overhead because it has to be assumed that the pointers may be aliased,
in which case  the caching layer will need to perform data lookups in all assigned cache lines for each memory access --- even if to memory directly.



\subsection{S-L1 overheads}
When evaluating S-L1, the following overheads need to be considered:


\noindent
{\bf Monitoring phase:} 
Our experiments show that the performance overhead of the monitoring phase is relatively low --- an average of less than 1\% was observed in the 10  applications we experimented with (see Section~\ref{}). 
The overhead is low because the monitoring phase only runs for a short period of time and because the code of \texttt{simulateCache()} is straightforward and typically does not incur additional memory accesses since all variables used in \texttt{simulateCache()} can be stored in statically allocated registers. 
In terms of register usage, the monitoring phase requires two registers per data structure/simulated cache line: one for the mapped address of the cache line in memory and one to keep the cache hit counter. 
These registers are only required during the monitoring phase and will be reused after
the phase terminates.

\noindent
{\bf calculateWhatDataToCache():} 
The performance overhead of \texttt{calculateWhatDataToCache()} is
negligible since it only needs to identify which $CLN$ have the highest hit counts
and typically the applications access only a few data structures.

\noindent
{\bf accessThroughCache():} 
The heavy lifting of the caching layer occurs in the \texttt{accessThroughCache()} function. 
For accesses to data structures that are not cached, the performance overhead entails
the execution of three extra machine instructions.
\todo{FACTCHECK}. 
However, accesses to cached data structures incur more significant overhead since a large body of code needs to be executed in some cases; e.g., when evicting a cache line.
Our experiments indicate that the caching layer increases the number of instructions issued by 25\% on average over the course of the entire application.
This overhead can indeed negatively impact the overall performance of an application if it is not amortized by the lower access times of accesses to S-L1, and the overhead is exacerbated if the application's throughput is already limited by instruction-issue bandwidth.

In terms of register usage, \texttt{accessThroughCache()} requires three additional registers per data structure and thread: one for the memory address of the data currently being cached, one for the write bitmap (which also serves as the dirty bit), and one for the data structure identifier.
(If the data structure is not cached, the value of the last register will be -1). 
As an optimization, we do not allocate bitmap registers for read-only data structures. Additionally, since data structures that are not cached do not access the bitmap and address registers, the compiler might spill them to memory, without accessing them later, thus reducing the register usage of un-cached data structures to 1.
The most recent GPU architectures (e.g. {\it Kepler} have 65,535 registers per SX and can support at most 2,048 threads, in which case the S-L1 caching layer would use up to 6\% and 9\% of the total number of available registers for cached read-only and read-write data structures, respectively. 
% For example, an application that has 2 read-only and 2 read-write data structures,
% needs to put aside up to 31\% of registers available to it for the caching layer.


\subsection{Coherence considerations}

Since each thread has its own private cache lines, cached data will not be coherent across
cache lines of different threads. 
Thus, if two threads write to the same data item cached separately, the correctness of the program might be compromised.
We follow two simple rules to maintain the correctness of the program, given the coherence model supported by the GPU:
(a)~we flush the entire cache on {\it memory fence instructions} and (b)~we do not cache the memory locations accessed by {atomic instructions}. 

Executing a {\it memory fence instruction} enforces all memory writes that were performed before the instruction to be visible to all other GPU threads before the execution of the next instruction. 
GPGPU programmers  are required to explicitly use these instructions if the application logic relies on a specific ordering of memory reads/writes. 
We implement this by inserting a call to \texttt{flush()} immediately before each memory fence instruction. 
As described earlier, \texttt{flush()}  flushes the contents of the modified cache lines to memory and invalidates the cache lines by setting their associated mapped addresses to zero.

By executing an {\it atomic instruction} a thread can read, modify, and write back a data in GPU memory atomically.
We insert an overloaded version of \texttt{flush()} before each atomic operation.
The overloaded version of \texttt{flush()} has two arguments: the address of the memory location accessed by the atomic operation and the associated data structure identifier.
It flushes the cache line if (and only if) the cache line contains the data pointed to by \texttt{address}~\cite{jia2012characterizing}.



